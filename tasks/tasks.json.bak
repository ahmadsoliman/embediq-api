{
  "tasks": [
    {
      "id": 1,
      "title": "Setup Docker Environment with PostgreSQL",
      "description": "Configure Docker Compose for development environment with the pre-configured PostgreSQL image that includes vector and AGE graph extensions.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "details": "Create a docker-compose.yml file in the project root that defines two services: (1) database using the eldoc92/postgres-rag-arm64:latest image with appropriate environment variables, volumes, and port mapping; (2) backend service that builds from a Dockerfile in the backend directory with appropriate volume mounts for code and user data. Define persistent volumes for postgres_data and user_data. Set environment variables for database connection, Auth0 configuration, and data directory paths.",
      "testStrategy": "Verify Docker Compose setup by running 'docker-compose up' and confirming both services start successfully. Test database connectivity from the backend container. Ensure volumes are properly mounted and persistent across container restarts.",
      "subtasks": [
        {
          "id": 1,
          "title": "Define Docker Compose Services and Persistent Volumes",
          "description": "Create a docker-compose.yml file in the project root that defines two services: a PostgreSQL database using the eldoc92/postgres-rag-arm64:latest image, and a backend service built from the backend directory. Set up persistent Docker volumes for postgres_data and user_data.",
          "status": "done",
          "dependencies": [],
          "details": "In the docker-compose.yml file, specify the version (e.g., '3.8'). Under 'services', define the 'database' service with the correct image, port mapping (e.g., 5432:5432), and mount the 'postgres_data' volume to /var/lib/postgresql/data. Define the 'backend' service to build from the backend/Dockerfile, and mount the 'user_data' volume to the appropriate path inside the container. Under 'volumes', declare both 'postgres_data' and 'user_data' as named volumes for data persistence.[1][3]\n\n<info added on 2025-04-20T11:29:08.167Z>\nHere's additional information for your Docker Compose setup:\n\n```yaml\n# Sample docker-compose.yml structure\nversion: '3.8'\n\nservices:\n  database:\n    image: eldoc92/postgres-rag-arm64:latest\n    environment:\n      - POSTGRES_USER=embediq\n      - POSTGRES_PASSWORD=devpassword\n      - POSTGRES_DATABASE=embediq\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U embediq\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n    restart: unless-stopped\n    \n  backend:\n    build: \n      context: ./backend\n    environment:\n      - DATABASE_URL=postgresql://embediq:devpassword@database:5432/embediq\n      - AUTH0_DOMAIN=${AUTH0_DOMAIN}\n      - AUTH0_API_AUDIENCE=${AUTH0_API_AUDIENCE}\n      - DATA_DIR=/data/embediq/users\n    restart: unless-stopped\n    depends_on:\n      database:\n        condition: service_healthy\n    networks:\n      - embediq-network\n\nnetworks:\n  embediq-network:\n    driver: bridge\n```\n\nConsider adding:\n1. A healthcheck for the database to ensure the backend only starts when Postgres is ready\n2. Restart policies for both services\n3. A dedicated bridge network for service communication\n4. Using environment variables for sensitive information\n5. The DATA_DIR environment variable should match the mounted user_data volume path\n\nFor local development, you might want to add a .env file in the project root to store AUTH0 credentials and other environment-specific variables.\n</info added on 2025-04-20T11:29:08.167Z>\n\n<info added on 2025-04-20T11:34:52.261Z>\n<info added>\nBased on the updated directory structure, here's how to adjust your Docker Compose configuration:\n\n```yaml\n# Updated build context for backend service\nservices:\n  backend:\n    build: \n      context: ./src  # Changed from ./backend to ./src\n    volumes:\n      - user_data:/data/embediq/users  # Ensure this volume mapping is present\n    # Other configuration remains the same\n```\n\nWhen working with the revised directory structure:\n\n1. Ensure your Dockerfile exists at `./src/Dockerfile` rather than `./backend/Dockerfile`\n2. Update any relative paths in your backend code that might reference the project root\n3. If you have any scripts that reference the backend directory, update them to use `src` instead\n4. Consider adding a `.dockerignore` file in the `src` directory to exclude unnecessary files from the build context\n\nThe change from `backend` to `src` is purely structural and doesn't affect the functionality of your services, but it's important to maintain consistency across your project documentation and references.\n</info added>\n</info added on 2025-04-20T11:34:52.261Z>"
        },
        {
          "id": 2,
          "title": "Configure Environment Variables for Services",
          "description": "Set up required environment variables for both the database and backend services in the docker-compose.yml file. This includes PostgreSQL credentials, vector and AGE extension settings, database connection details, Auth0 configuration, and data directory paths.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "In the 'environment' section for the database service, define variables such as POSTGRES_USER, POSTGRES_PASSWORD, and POSTGRES_DATABASE. If the vector and AGE extensions require additional variables or initialization scripts, include those as well. For the backend service, set environment variables for connecting to the database (host, port, user, password, db), Auth0 client and domain, and any paths needed for user data. Use Docker Compose variable substitution or .env files as appropriate for sensitive data.[1][3][4]\n\n<info added on 2025-04-20T11:35:32.534Z>\nAdd the following to the details section:\n\nFor better security and developer onboarding, create a `.env.example` file in the project root with all required variables:\n\n```\n# Database configuration\nPOSTGRES_USER=postgres\nPOSTGRES_PASSWORD=your_secure_password\nPOSTGRES_DATABASE=knowledge_base\nPOSTGRES_PORT=5432\n\n# Vector extension settings\nVECTOR_EXTENSION_ENABLED=true\nVECTOR_DIMENSION=1536\n\n# AGE extension settings\nAGE_EXTENSION_ENABLED=true\n\n# Backend connection string (constructed from above values)\nDATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DATABASE}\n\n# Auth0 configuration\nAUTH0_DOMAIN=your-tenant.auth0.com\nAUTH0_API_AUDIENCE=https://your-api-identifier\n\n# Data storage\nDATA_DIR=/app/data\n```\n\nDevelopers should copy this to a `.env` file and customize values. The docker-compose.yml is already correctly configured to use variable substitution (${VAR_NAME}) which will pull values from this .env file at runtime. No additional changes to docker-compose.yml are needed as the environment variables are properly set up.\n</info added on 2025-04-20T11:35:32.534Z>"
        },
        {
          "id": 3,
          "title": "Implement Backend Service Build and Integration",
          "description": "Create or update the Dockerfile in the backend directory to build the backend service image. Ensure the Dockerfile supports code and user data volume mounts. Test the integration between the backend and the PostgreSQL service with the configured extensions.",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "Write or modify the backend/Dockerfile to install all necessary dependencies and expose the correct ports. Ensure the backend service can access the mounted user_data volume and connect to the database using the environment variables defined in the previous step. After building the images with 'docker-compose up', verify that the backend can connect to the database and that the vector and AGE extensions are available and functional.\n\n<info added on 2025-04-20T11:36:55.367Z>\nFor the backend Dockerfile implementation, here's additional technical information:\n\n```\n# Create the following directory structure:\nsrc/\n├── Dockerfile\n├── requirements.txt\n├── app/\n│   ├── __init__.py\n│   ├── main.py\n│   ├── config.py\n│   └── db/\n│       ├── __init__.py\n│       └── connection.py\n\n# requirements.txt should include:\nfastapi>=0.95.0\nuvicorn>=0.21.1\npsycopg2-binary>=2.9.6\npython-dotenv>=1.0.0\nsqlalchemy>=2.0.9\nasyncpg>=0.27.0\npgvector>=0.1.8\n\n# In the Dockerfile, include health check configuration:\nHEALTHCHECK --interval=30s --timeout=5s --start-period=5s --retries=3 \\\n  CMD curl -f http://localhost:8000/health || exit 1\n\n# In app/config.py, implement environment variable handling:\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nDATABASE_URL = os.getenv(\"DATABASE_URL\", \"postgresql://postgres:postgres@db:5432/postgres\")\nVECTOR_DIMENSION = int(os.getenv(\"VECTOR_DIMENSION\", \"1536\"))\n\n# In app/db/connection.py, implement database connection with extension verification:\nfrom sqlalchemy import create_engine, text\nfrom app.config import DATABASE_URL\n\ndef verify_extensions():\n    \"\"\"Verify that required PostgreSQL extensions are available and enabled\"\"\"\n    engine = create_engine(DATABASE_URL)\n    with engine.connect() as conn:\n        # Check vector extension\n        vector_result = conn.execute(text(\"SELECT * FROM pg_extension WHERE extname = 'vector'\")).fetchone()\n        # Check AGE extension\n        age_result = conn.execute(text(\"SELECT * FROM pg_extension WHERE extname = 'age'\")).fetchone()\n        \n        if not vector_result or not age_result:\n            raise RuntimeError(\"Required PostgreSQL extensions are not enabled\")\n        \n        return True\n```\n\nFor volume mounting, ensure the Dockerfile creates and sets permissions for the data directory:\n```\n# In Dockerfile\nRUN mkdir -p /app/data && chmod 777 /app/data\nVOLUME [\"/app/data\"]\n```\n\nWhen testing the integration, use this verification script in app/main.py:\n```python\n@app.get(\"/health\")\nasync def health_check():\n    try:\n        # Verify database connection and extensions\n        from app.db.connection import verify_extensions\n        extensions_ok = verify_extensions()\n        return {\"status\": \"healthy\", \"database\": \"connected\", \"extensions\": extensions_ok}\n    except Exception as e:\n        return {\"status\": \"unhealthy\", \"error\": str(e)}, 500\n```\n</info added on 2025-04-20T11:36:55.367Z>"
        }
      ]
    },
    {
      "id": 2,
      "title": "Create Backend Dockerfile and Project Structure",
      "description": "Set up the backend project structure and create a Dockerfile for containerization.",
      "status": "done",
      "dependencies": [
        1
      ],
      "priority": "high",
      "details": "Create a Dockerfile in the backend directory using python:3.10-slim as the base image. Set up the project structure with appropriate directories for routes, middleware, services, and utilities. Initialize a Python FastAPI project with requirements.txt including FastAPI, uvicorn, python-jose for JWT handling, httpx for HTTP requests, and any other required dependencies. Create the main application entry point that initializes FastAPI. Set up health check endpoint for container monitoring.",
      "testStrategy": "Build the Docker image and verify it runs successfully. Test the health check endpoint. Ensure the project structure follows best practices and all dependencies are correctly specified in requirements.txt.",
      "subtasks": [
        {
          "id": 1,
          "title": "Initialize Backend Project Structure",
          "description": "Set up the backend directory with a clear and scalable FastAPI project structure, including subdirectories for routes, middleware, services, and utilities.",
          "status": "done",
          "dependencies": [],
          "details": "Create the backend root directory. Inside, add subdirectories: routes (for API endpoints), middleware (for custom middleware), services (for business logic), and utilities (for helper functions). Add an __init__.py file in each directory to make them Python packages. Create a main.py file at the root as the application entry point. Follow best practices for modular FastAPI applications[2][5]."
        },
        {
          "id": 2,
          "title": "Initialize FastAPI Application and Health Check Endpoint",
          "description": "Set up the FastAPI application in main.py, configure the app to include a health check endpoint, and prepare requirements.txt with all necessary dependencies.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "In main.py, initialize a FastAPI app instance. Implement a basic health check endpoint (e.g., /health) that returns a simple JSON response for container monitoring. Create a requirements.txt file listing FastAPI, uvicorn, python-jose, httpx, and any other required packages. Ensure the app imports and registers routes, middleware, and other components as needed[2][5]."
        },
        {
          "id": 3,
          "title": "Create Dockerfile for Backend Containerization",
          "description": "Write a Dockerfile in the backend directory to containerize the FastAPI application using python:3.10-slim as the base image.",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "In the backend directory, create a Dockerfile that uses python:3.10-slim as the base image. Set the working directory, copy project files, install dependencies from requirements.txt, expose the appropriate port (e.g., 80), and define the CMD to start the FastAPI app with uvicorn. Ensure the Dockerfile follows best practices for Python and FastAPI containerization[1][3][4]."
        }
      ]
    },
    {
      "id": 3,
      "title": "Implement Auth0 Integration and Authentication Middleware",
      "description": "Create middleware for Auth0 JWT validation and user authentication.",
      "status": "done",
      "dependencies": [
        2
      ],
      "priority": "high",
      "details": "Implement authentication middleware that extracts and validates JWT tokens from request headers. Use python-jose to decode and verify tokens against Auth0 public keys. Extract user ID from the validated token. Create helper functions to handle token validation errors and return appropriate HTTP exceptions. Implement endpoints for token validation (/api/v1/auth/token) and user profile retrieval (/api/v1/auth/profile). Store Auth0 configuration in environment variables.",
      "testStrategy": "Write unit tests for token validation with mock JWT tokens. Test error handling for invalid or expired tokens. Manually test authentication flow with Auth0 development credentials. Verify correct extraction of user identifiers from tokens.",
      "subtasks": [
        {
          "id": 1,
          "title": "Configure Auth0 Environment and Helper Functions",
          "description": "Set up environment variables for Auth0 configuration and implement helper functions for JWT validation and error handling.",
          "status": "done",
          "dependencies": [],
          "details": "Define and load Auth0-related environment variables (domain, audience, client ID, public key URL). Implement helper functions using python-jose to fetch Auth0 public keys, decode and verify JWT tokens, and handle validation errors by returning appropriate HTTP exceptions. Ensure all sensitive configuration is securely managed via environment variables."
        },
        {
          "id": 2,
          "title": "Develop Authentication Middleware for JWT Extraction and Validation",
          "description": "Create middleware that extracts JWT tokens from request headers, validates them using the helper functions, and attaches user information to the request context.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Implement middleware that intercepts incoming requests, extracts the Authorization header, and retrieves the JWT token. Use the previously created helper functions to decode and validate the token against Auth0 public keys. On successful validation, extract the user ID and attach it to the request context for downstream use. Handle and propagate any validation errors using the error handling helpers."
        },
        {
          "id": 3,
          "title": "Implement Auth Endpoints for Token Validation and User Profile Retrieval",
          "description": "Develop the /api/v1/auth/token and /api/v1/auth/profile endpoints, utilizing the authentication middleware to validate tokens and return user information.",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "Create the /api/v1/auth/token endpoint to validate the provided JWT and return a success or error response. Implement the /api/v1/auth/profile endpoint to return the authenticated user's profile information, using the user ID extracted by the middleware. Ensure both endpoints are protected by the authentication middleware and return standardized HTTP responses."
        }
      ]
    },
    {
      "id": 4,
      "title": "Implement RAG Instance Manager",
      "description": "Create a manager class to handle LightRAG instances for multiple users with LRU caching.",
      "status": "done",
      "dependencies": [
        2
      ],
      "priority": "high",
      "details": "Implement the RAGInstanceManager class that maintains a dictionary of user-specific LightRAG instances. Add the LRURAGManager subclass that implements least-recently-used caching to limit memory usage. Implement methods to create, retrieve, and clean up LightRAG instances. Configure the manager to create user-specific directories under the base data path. Initialize LightRAG with appropriate configuration including working directory, LLM model function, embedding function, and database connection string.",
      "testStrategy": "Write unit tests to verify instance creation, retrieval, and LRU eviction behavior. Test with multiple simulated users to ensure proper isolation. Verify directory creation and permissions. Measure memory usage with different numbers of active instances.",
      "subtasks": [
        {
          "id": 1,
          "title": "Design and Implement RAGInstanceManager Base Class",
          "description": "Develop the RAGInstanceManager class to manage user-specific LightRAG instances, including methods for creating, retrieving, and cleaning up instances. Ensure each user has a dedicated directory under the base data path, and initialize LightRAG with the required configuration parameters.",
          "status": "done",
          "dependencies": [],
          "details": "Define a RAGInstanceManager class that maintains a dictionary mapping user IDs to LightRAG instances. Implement methods such as create_instance(user_id), get_instance(user_id), and cleanup_instance(user_id). When creating an instance, ensure a user-specific directory is created under the base data path. Initialize each LightRAG instance with the appropriate working directory, LLM model function, embedding function, and database connection string."
        },
        {
          "id": 2,
          "title": "Implement LRURAGManager Subclass with LRU Caching",
          "description": "Extend RAGInstanceManager by creating an LRURAGManager subclass that adds least-recently-used (LRU) caching to limit the number of active LightRAG instances and manage memory usage efficiently.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Create the LRURAGManager subclass, inheriting from RAGInstanceManager. Integrate an LRU cache mechanism (e.g., using collections.OrderedDict or functools.lru_cache) to automatically evict the least recently used LightRAG instances when the cache limit is reached. Ensure that evicted instances are properly cleaned up by invoking the cleanup_instance method."
        },
        {
          "id": 3,
          "title": "Integrate and Test User-Specific Instance Management Workflow",
          "description": "Combine the manager and LRU caching logic into a unified workflow. Implement and test the full lifecycle: creating, retrieving, and cleaning up LightRAG instances for multiple users, ensuring correct directory setup and resource management.",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "Write integration tests or scripts that simulate multiple users interacting with the manager. Verify that user-specific directories are created, instances are initialized with correct configurations, and the LRU cache properly evicts and cleans up old instances. Confirm that all methods work as expected and that memory usage remains within limits."
        }
      ]
    },
    {
      "id": 5,
      "title": "Create User-Specific LightRAG Initialization",
      "description": "Implement functions to initialize and configure LightRAG instances for each user.",
      "status": "done",
      "dependencies": [
        4
      ],
      "priority": "high",
      "details": "Create utility functions to initialize LightRAG with user-specific working directories. Implement the get_llm_model_func() and get_embedding_func() helper functions that return properly configured model functions for LightRAG. Set up the embedding function with the required 1536 dimension and 8192 max token size as specified in the PRD. Create a dependency injection function (get_rag_for_user) that extracts the user ID from the request and returns the appropriate LightRAG instance using the RAG Instance Manager.",
      "testStrategy": "Test initialization with different user IDs to ensure unique working directories are created. Verify embedding and LLM model functions are correctly configured. Test the dependency injection in a mock FastAPI endpoint to ensure it correctly retrieves user-specific RAG instances.",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement User-Specific LightRAG Initialization Utility",
          "description": "Create a utility function that initializes a LightRAG instance with a user-specific working directory. This function should accept a user identifier and set up the LightRAG instance to use a dedicated directory for each user, ensuring data separation and proper resource allocation.",
          "status": "done",
          "dependencies": [],
          "details": "Define a function (e.g., initialize_lightrag_for_user(user_id)) that constructs a unique working directory path for each user (such as './rag_storage/{user_id}'). Use this path to initialize the LightRAG instance. Ensure that the directory is created if it does not exist. Reference the LightRAG initialization pattern from the official documentation, passing the working_dir parameter accordingly[1][2]."
        },
        {
          "id": 2,
          "title": "Develop Model and Embedding Helper Functions",
          "description": "Implement the get_llm_model_func() and get_embedding_func() helper functions to return properly configured model and embedding functions for LightRAG. Ensure the embedding function is set up with a 1536-dimensional vector and supports up to 8192 tokens, as specified in the PRD.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Define get_llm_model_func() to return the appropriate LLM completion function (e.g., gpt_4o_mini_complete or similar). Define get_embedding_func() to return an embedding function configured for 1536 dimensions and 8192 max token size. If using a built-in embedding function (such as openai_embed), ensure it is parameterized or wrapped to enforce these requirements[1]."
        },
        {
          "id": 3,
          "title": "Implement Dependency Injection for User-Specific RAG Instances",
          "description": "Create a dependency injection function (get_rag_for_user) that extracts the user ID from the incoming request and returns the corresponding LightRAG instance using a RAG Instance Manager. This function should ensure that each user receives a properly initialized and configured LightRAG instance.",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "Implement get_rag_for_user(request) to extract the user ID (e.g., from request headers or session). Use a RAG Instance Manager (such as a singleton or cache) to retrieve or initialize the LightRAG instance for that user, leveraging the utility and helper functions from the previous subtasks. Ensure thread safety and efficient instance reuse where appropriate[2]."
        }
      ]
    },
    {
      "id": 6,
      "title": "Implement Document Management API Endpoints",
      "description": "Create API endpoints for document upload and management using LightRAG.",
      "status": "done",
      "dependencies": [
        5
      ],
      "priority": "medium",
      "details": "Implement the /api/v1/documents endpoint with POST method for document uploads. Create a temporary file storage mechanism for handling uploaded files before processing with LightRAG. Use the user's LightRAG instance to process and store documents. Implement proper error handling and cleanup of temporary files. Add endpoints for listing, retrieving, and deleting documents. Ensure all operations use the user-specific LightRAG instance obtained through dependency injection.",
      "testStrategy": "Test document upload with various file types and sizes. Verify documents are correctly processed by LightRAG and stored in user-specific directories. Test error handling for invalid files. Verify temporary files are properly cleaned up after processing. Test document listing and retrieval to ensure only the user's own documents are accessible.",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement Document Upload Endpoint with Temporary File Storage",
          "description": "Create the /api/v1/documents POST endpoint to handle document uploads, storing files temporarily before processing with LightRAG.",
          "status": "done",
          "dependencies": [],
          "details": "Develop the POST /api/v1/documents endpoint to accept file uploads. Implement a temporary file storage mechanism—such as saving files to a temp directory or using an in-memory store—ensuring files are accessible for subsequent processing. Integrate error handling for upload failures and ensure temporary files are cleaned up after processing or on error. Use dependency injection to access the user-specific LightRAG instance for later processing steps."
        },
        {
          "id": 2,
          "title": "Integrate LightRAG Processing and Persistent Document Storage",
          "description": "Process uploaded documents using the user-specific LightRAG instance and store processed documents persistently.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "After successful upload and temporary storage, invoke the LightRAG instance (retrieved via dependency injection) to process the uploaded document. Store the processed document and its metadata in a persistent storage solution (e.g., database or object storage). Ensure that temporary files are deleted after processing, and implement robust error handling to manage failures during processing or storage."
        },
        {
          "id": 3,
          "title": "Implement Document Listing, Retrieval, and Deletion Endpoints",
          "description": "Create endpoints for listing all documents, retrieving a specific document, and deleting documents, ensuring all operations use the user-specific LightRAG instance.",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "Develop GET /api/v1/documents for listing documents, GET /api/v1/documents/{id} for retrieving a specific document, and DELETE /api/v1/documents/{id} for deleting documents. Ensure each endpoint interacts with the persistent storage and uses the user-specific LightRAG instance for any required document operations. Implement proper error handling and return appropriate HTTP status codes for all operations."
        }
      ]
    },
    {
      "id": 7,
      "title": "Implement Query and Search API Endpoints",
      "description": "Create API endpoints for vector similarity search and RAG-powered queries.",
      "status": "done",
      "dependencies": [
        5
      ],
      "priority": "medium",
      "details": "Implement the /api/v1/search endpoint for vector similarity search using LightRAG. Create the /api/v1/query endpoint for combined RAG-powered queries. Define request and response models for both endpoints. Implement parameter validation and error handling. Ensure all queries use the user-specific LightRAG instance. Add options for configuring search parameters like maximum chunks to retrieve and similarity thresholds.",
      "testStrategy": "Test search and query endpoints with various input parameters. Verify results are correctly returned from LightRAG. Test with different user instances to ensure data isolation. Measure response times for different query complexities. Test error handling for invalid queries.",
      "subtasks": [
        {
          "id": 1,
          "title": "Define Request and Response Models with Parameter Validation",
          "description": "Design and implement the data models for both /api/v1/search and /api/v1/query endpoints, including all required and optional parameters. Add validation logic for input parameters such as query vectors, maximum chunks, and similarity thresholds. Ensure models are structured for clear API documentation and future extensibility.",
          "status": "done",
          "dependencies": [],
          "details": "Use a schema validation library (e.g., Pydantic for Python or equivalent) to define request and response models. Specify fields for query vectors, search parameters (e.g., similarity function, max chunks, thresholds), and user identification. Implement validation to enforce correct types, required fields, and value ranges. Prepare error response models for invalid input."
        },
        {
          "id": 2,
          "title": "Implement /api/v1/search Endpoint for Vector Similarity Search",
          "description": "Develop the /api/v1/search endpoint to perform vector similarity search using the user-specific LightRAG instance. Integrate the search logic to retrieve relevant chunks based on the provided query vector and configurable search parameters.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Set up the endpoint route and handler. Retrieve the user-specific LightRAG instance based on the authenticated user. Use the validated request model to extract the query vector and search parameters. Call the vector similarity search function, passing in parameters such as max chunks and similarity thresholds. Return results using the defined response model, and handle errors gracefully."
        },
        {
          "id": 3,
          "title": "Implement /api/v1/query Endpoint for Combined RAG-Powered Queries",
          "description": "Develop the /api/v1/query endpoint to handle retrieval-augmented generation (RAG) queries, combining vector search with generative responses. Ensure the endpoint uses the user-specific LightRAG instance and supports configurable search parameters.",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "Set up the endpoint route and handler. Use the validated request model to extract the user query and search parameters. Perform a vector similarity search to retrieve relevant context, then pass the context and query to the RAG pipeline for answer generation. Return the generated response and supporting context using the defined response model. Implement robust error handling for both search and generation steps."
        }
      ]
    },
    {
      "id": 8,
      "title": "Implement Knowledge Graph API Endpoints",
      "description": "Create API endpoints for knowledge graph operations using LightRAG.",
      "status": "done",
      "dependencies": [
        5
      ],
      "priority": "medium",
      "details": "Implement the /api/v1/graph endpoint for knowledge graph operations. Create methods for retrieving graph structure, adding relationships, and performing graph traversals. Define request and response models for graph operations. Ensure all operations use the user-specific LightRAG instance. Implement visualization-friendly response formats for graph data.",
      "testStrategy": "Test graph endpoints with various input parameters. Verify graph operations correctly modify or retrieve data from LightRAG. Test with different user instances to ensure data isolation. Verify graph data is correctly formatted for visualization.",
      "subtasks": [
        {
          "id": 1,
          "title": "Define Knowledge Graph API Models and Schemas",
          "description": "Create Pydantic models for all knowledge graph entities, relationships, and API request/response schemas to ensure proper validation and serialization.",
          "dependencies": [],
          "details": "Implementation details:\n1. Create base models for nodes, edges, and properties using Pydantic\n2. Define request models for operations (create, update, search, traverse)\n3. Define response models with visualization-friendly formats (JSON-LD compatible)\n4. Implement field selection capability in response models\n5. Add validation rules to ensure data integrity\n6. Create documentation strings for OpenAPI/Swagger\n\nTesting approach:\n- Unit test each model with valid and invalid data\n- Verify serialization/deserialization works correctly\n- Test field selection functionality\n\nExample code:\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Dict, List, Optional, Any\n\nclass GraphNode(BaseModel):\n    id: str\n    label: str\n    properties: Dict[str, Any] = Field(default_factory=dict)\n\nclass GraphEdge(BaseModel):\n    source: str\n    target: str\n    type: str\n    properties: Dict[str, Any] = Field(default_factory=dict)\n\nclass GraphResponse(BaseModel):\n    nodes: List[GraphNode]\n    edges: List[GraphEdge]\n    \n    class Config:\n        schema_extra = {\n            \"example\": {\n                \"nodes\": [{\"id\": \"1\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"}}],\n                \"edges\": [{\"source\": \"1\", \"target\": \"2\", \"type\": \"KNOWS\", \"properties\": {}}]\n            }\n        }\n```",
          "status": "done",
          "parentTaskId": 8
        },
        {
          "id": 2,
          "title": "Implement Graph Structure Retrieval Endpoints",
          "description": "Create API endpoints for retrieving knowledge graph structure, including nodes, edges, and subgraphs with filtering and pagination support.",
          "dependencies": [
            1
          ],
          "details": "Implementation details:\n1. Create GET `/api/v1/graph` endpoint to retrieve the full graph structure\n2. Implement GET `/api/v1/graph/nodes` endpoint with filtering by label and properties\n3. Implement GET `/api/v1/graph/edges` endpoint with filtering by type\n4. Add pagination support using limit/offset query parameters\n5. Implement field selection via query parameters\n6. Ensure all endpoints use the user-specific LightRAG instance\n\nTesting approach:\n- Test with various filter combinations\n- Verify pagination works correctly\n- Test with large datasets to ensure performance\n- Verify field selection reduces response size appropriately\n\nExample code:\n```python\nfrom fastapi import FastAPI, Depends, Query, HTTPException\nfrom typing import List, Optional\nfrom .models import GraphNode, GraphEdge, GraphResponse\nfrom .dependencies import get_user_lightrag\n\napp = FastAPI()\n\n@app.get(\"/api/v1/graph\", response_model=GraphResponse)\ndef get_graph(\n    limit: int = Query(10, ge=1, le=100),\n    offset: int = Query(0, ge=0),\n    fields: Optional[str] = None,\n    lightrag = Depends(get_user_lightrag)\n):\n    # Get graph data from user's LightRAG instance\n    nodes, edges = lightrag.get_graph(limit=limit, offset=offset)\n    \n    # Apply field selection if specified\n    if fields:\n        field_list = fields.split(\",\")\n        # Filter fields logic here\n    \n    return GraphResponse(nodes=nodes, edges=edges)\n```",
          "status": "done",
          "parentTaskId": 8
        },
        {
          "id": 3,
          "title": "Implement Graph Modification Endpoints",
          "description": "Create API endpoints for adding, updating, and deleting nodes and relationships in the knowledge graph.",
          "dependencies": [
            1
          ],
          "details": "Implementation details:\n1. Implement POST `/api/v1/graph/nodes` to create new nodes\n2. Implement POST `/api/v1/graph/edges` to create new relationships\n3. Implement PUT/PATCH endpoints for updating nodes and edges\n4. Implement DELETE endpoints for removing nodes and edges\n5. Add transaction support to ensure data consistency\n6. Return appropriate status codes (201 for creation, 204 for deletion)\n\nTesting approach:\n- Test CRUD operations individually\n- Test error handling for invalid inputs\n- Test transaction rollback on failure\n- Verify proper status codes are returned\n\nExample code:\n```python\n@app.post(\"/api/v1/graph/nodes\", response_model=GraphNode, status_code=201)\ndef create_node(\n    node: GraphNode,\n    lightrag = Depends(get_user_lightrag)\n):\n    try:\n        created_node = lightrag.create_node(node.dict())\n        return created_node\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.delete(\"/api/v1/graph/nodes/{node_id}\", status_code=204)\ndef delete_node(\n    node_id: str,\n    lightrag = Depends(get_user_lightrag)\n):\n    try:\n        lightrag.delete_node(node_id)\n        return None\n    except KeyError:\n        raise HTTPException(status_code=404, detail=\"Node not found\")\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=str(e))\n```",
          "status": "done",
          "parentTaskId": 8
        },
        {
          "id": 4,
          "title": "Implement Graph Traversal and Search Endpoints",
          "description": "Create API endpoints for traversing the graph, finding paths between nodes, and performing semantic searches within the knowledge graph.",
          "dependencies": [
            1,
            2
          ],
          "details": "Implementation details:\n1. Implement GET `/api/v1/graph/traverse` endpoint for graph traversal operations\n2. Create GET `/api/v1/graph/paths` endpoint to find paths between nodes\n3. Implement GET `/api/v1/graph/search` for semantic search within the graph\n4. Add query parameters for controlling traversal depth and direction\n5. Implement filtering by node types and edge types\n6. Ensure efficient traversal algorithms are used\n\nTesting approach:\n- Test traversal with different depths and directions\n- Test path finding between various node pairs\n- Benchmark performance with large graphs\n- Test semantic search with various queries\n\nExample code:\n```python\n@app.get(\"/api/v1/graph/traverse\", response_model=GraphResponse)\ndef traverse_graph(\n    start_node: str,\n    direction: str = Query(\"outbound\", enum=[\"outbound\", \"inbound\", \"any\"]),\n    max_depth: int = Query(2, ge=1, le=5),\n    edge_types: Optional[str] = None,\n    lightrag = Depends(get_user_lightrag)\n):\n    edge_type_list = edge_types.split(\",\") if edge_types else None\n    \n    subgraph = lightrag.traverse(\n        start_node=start_node,\n        direction=direction,\n        max_depth=max_depth,\n        edge_types=edge_type_list\n    )\n    \n    return GraphResponse(nodes=subgraph[\"nodes\"], edges=subgraph[\"edges\"])\n\n@app.get(\"/api/v1/graph/search\", response_model=GraphResponse)\ndef search_graph(\n    query: str,\n    limit: int = Query(10, ge=1, le=100),\n    lightrag = Depends(get_user_lightrag)\n):\n    results = lightrag.search(query=query, limit=limit)\n    return GraphResponse(nodes=results[\"nodes\"], edges=results[\"edges\"])\n```",
          "status": "done",
          "parentTaskId": 8
        },
        {
          "id": 5,
          "title": "Implement Visualization-Friendly Response Formats and Documentation",
          "description": "Enhance API responses with visualization-friendly formats and create comprehensive API documentation using OpenAPI/Swagger.",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "Implementation details:\n1. Implement JSON-LD compatible response format for interoperability\n2. Add specialized visualization endpoints that return data in formats compatible with common graph visualization libraries (D3.js, Cytoscape.js)\n3. Create custom response serializers for different visualization formats\n4. Implement content negotiation to support multiple response formats\n5. Create comprehensive OpenAPI documentation with examples\n6. Add rate limiting and caching headers for performance\n\nTesting approach:\n- Verify response formats work with visualization libraries\n- Test content negotiation with different Accept headers\n- Validate JSON-LD responses against schema\n- Test documentation endpoints return valid OpenAPI specs\n\nExample code:\n```python\nfrom fastapi import Response, Header\nfrom typing import Optional\n\n@app.get(\"/api/v1/graph/visualize\", response_model=None)\ndef get_visualization_data(\n    format: str = Query(\"d3\", enum=[\"d3\", \"cytoscape\", \"jsonld\"]),\n    lightrag = Depends(get_user_lightrag),\n    accept: Optional[str] = Header(None)\n):\n    # Get graph data\n    graph_data = lightrag.get_graph()\n    \n    # Determine format based on Accept header or query param\n    if accept and \"application/ld+json\" in accept:\n        format = \"jsonld\"\n    \n    # Format data according to requested visualization format\n    if format == \"d3\":\n        response_data = format_for_d3(graph_data)\n        return response_data\n    elif format == \"cytoscape\":\n        response_data = format_for_cytoscape(graph_data)\n        return response_data\n    elif format == \"jsonld\":\n        response_data = format_for_jsonld(graph_data)\n        return Response(\n            content=response_data,\n            media_type=\"application/ld+json\"\n        )\n\ndef format_for_d3(graph_data):\n    # Transform to D3.js format\n    return {\n        \"nodes\": [{\"id\": n.id, \"label\": n.label, **n.properties} for n in graph_data[\"nodes\"]],\n        \"links\": [{\"source\": e.source, \"target\": e.target, \"type\": e.type} for e in graph_data[\"edges\"]]\n    }\n```",
          "status": "done",
          "parentTaskId": 8
        }
      ]
    },
    {
      "id": 9,
      "title": "Implement Data Source Configuration API",
      "description": "Create API endpoints for configuring external data sources.",
      "status": "done",
      "dependencies": [
        6
      ],
      "priority": "low",
      "details": "Implement the /api/v1/datasources endpoint for configuring external data sources. Create methods for adding, updating, and removing data source configurations. Define request and response models for data source operations. Ensure configurations are stored in the user's working directory. Implement validation for different data source types.",
      "testStrategy": "Test data source configuration endpoints with various input parameters. Verify configurations are correctly stored in user-specific directories. Test validation of different data source types. Verify configurations persist across application restarts.",
      "subtasks": [
        {
          "id": 1,
          "title": "Define Data Source Configuration Models",
          "description": "Create Pydantic models for data source configurations that support different types of data sources and include proper validation. These models will be used for request/response handling and validation throughout the API.",
          "dependencies": [],
          "details": "1. Create a base DataSourceConfig Pydantic model with common fields:\n   - id (UUID, auto-generated)\n   - name (required, string)\n   - type (required, string - e.g., 'postgres', 'mysql', 'csv', 'api')\n   - description (optional, string)\n   - created_at and updated_at timestamps\n2. Create specific models for each data source type that inherit from the base model:\n   - DatabaseDataSource (for SQL databases with connection string, username, password fields)\n   - FileDataSource (for CSV, JSON files with path, format fields)\n   - APIDataSource (for REST APIs with url, auth_type, headers fields)\n3. Implement validation logic for each model type (e.g., validate connection strings, file paths)\n4. Ensure sensitive fields like passwords use SecretStr type for security\n5. Add helper methods for serialization/deserialization\n6. Write unit tests for model validation with various input scenarios",
          "status": "done",
          "parentTaskId": 9
        },
        {
          "id": 2,
          "title": "Implement Configuration Storage Service",
          "description": "Create a service layer to handle the storage and retrieval of data source configurations in the user's working directory. This service will handle persistence, encryption of sensitive data, and file operations.",
          "dependencies": [
            1
          ],
          "details": "1. Create a ConfigurationStorageService class with methods for CRUD operations:\n   - save_config(config: DataSourceConfig) -> DataSourceConfig\n   - get_config(config_id: str) -> DataSourceConfig\n   - list_configs() -> List[DataSourceConfig]\n   - update_config(config_id: str, config: DataSourceConfig) -> DataSourceConfig\n   - delete_config(config_id: str) -> None\n2. Implement file-based storage in the user's working directory:\n   - Use a subdirectory like '.datasources' in the user's working directory\n   - Store each configuration as a separate JSON file named by its ID\n   - Use the 'cryptography' library to encrypt sensitive fields before storage\n3. Handle file locking for concurrent access\n4. Implement error handling for file operations (permissions, disk space)\n5. Add logging for operations (excluding sensitive data)\n6. Write unit tests with mocked file system\n7. Ensure backward compatibility for configuration format changes",
          "status": "done",
          "parentTaskId": 9
        },
        {
          "id": 3,
          "title": "Create Data Source Validation Service",
          "description": "Implement a service to validate data source configurations by testing connections and verifying access permissions. This ensures that stored configurations are valid and usable.",
          "dependencies": [
            1
          ],
          "details": "1. Create a DataSourceValidationService with methods for different data source types:\n   - validate_database_connection(config: DatabaseDataSource) -> ValidationResult\n   - validate_file_access(config: FileDataSource) -> ValidationResult\n   - validate_api_endpoint(config: APIDataSource) -> ValidationResult\n2. Implement connection testing logic for each data source type:\n   - For databases: Establish connection, run simple query, check permissions\n   - For files: Check file existence, read permissions, parse header\n   - For APIs: Send test request, verify response format\n3. Create a ValidationResult model with fields for success status, error messages, and warnings\n4. Implement timeout handling to prevent long-running validation attempts\n5. Add detailed error messages for common configuration issues\n6. Create mock data sources for testing without actual external dependencies\n7. Write integration tests for each validation method with test data sources",
          "status": "done",
          "parentTaskId": 9
        },
        {
          "id": 4,
          "title": "Implement Data Source Configuration API Endpoints",
          "description": "Create the RESTful API endpoints for managing data source configurations, following best practices for API design, error handling, and security.",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "1. Implement the following endpoints using FastAPI:\n   - POST /api/v1/datasources - Create new data source configuration\n   - GET /api/v1/datasources - List all data source configurations\n   - GET /api/v1/datasources/{id} - Get specific data source configuration\n   - PUT /api/v1/datasources/{id} - Update data source configuration\n   - DELETE /api/v1/datasources/{id} - Delete data source configuration\n   - POST /api/v1/datasources/{id}/validate - Validate a data source configuration\n2. Add proper request/response models using the Pydantic models from subtask 1\n3. Implement filtering, sorting, and pagination for the list endpoint\n4. Add proper error handling with appropriate HTTP status codes\n5. Implement authentication and authorization middleware\n6. Add rate limiting to prevent abuse\n7. Ensure sensitive data is never returned in responses (mask credentials)\n8. Document API endpoints using OpenAPI/Swagger annotations\n9. Write integration tests for each endpoint",
          "status": "done",
          "parentTaskId": 9
        },
        {
          "id": 5,
          "title": "Implement Data Source Type Registry and Extension System",
          "description": "Create a registry system that allows for adding new data source types and their specific validation logic. This makes the system extensible for future data source types.",
          "dependencies": [
            1,
            3,
            4
          ],
          "details": "1. Create a DataSourceTypeRegistry class to manage available data source types:\n   - register_type(type_name: str, model_class, validator_class) -> None\n   - get_type(type_name: str) -> Tuple[model_class, validator_class]\n   - list_types() -> List[str]\n2. Implement a plugin system to load data source type definitions from external modules\n3. Create an extension interface with required methods for new data source types:\n   - get_type_name() -> str\n   - get_model_class() -> Type[DataSourceConfig]\n   - get_validator_class() -> Type[Validator]\n   - get_connection_parameters() -> List[Dict]\n4. Update API endpoints to use the registry for validation and model selection\n5. Add an endpoint to discover available data source types:\n   - GET /api/v1/datasources/types - List all supported data source types and their parameters\n6. Create documentation for extending the system with new data source types\n7. Implement at least one additional data source type as an example extension\n8. Write tests for the registry and extension loading system",
          "status": "done",
          "parentTaskId": 9
        }
      ]
    },
    {
      "id": 10,
      "title": "Implement Resource Management and Monitoring",
      "description": "Add resource management, monitoring, and backup capabilities to the system.",
      "status": "done",
      "dependencies": [
        4,
        5,
        6,
        7,
        8,
        9
      ],
      "priority": "low",
      "details": "Implement monitoring endpoints to track system resource usage. Add logging for critical operations and errors. Implement the backup strategy outlined in the PRD, including regular snapshots of user data directories and database backups. Create utilities for data replication across availability zones. Implement performance monitoring for LightRAG operations. Add configuration options for tuning LightRAG parameters like chunk sizes, embedding model parameters, and graph traversal depth.",
      "testStrategy": "Test monitoring endpoints to ensure accurate resource reporting. Verify backup and restore functionality with sample data. Test performance under various load conditions. Verify logs contain appropriate information for debugging and auditing.",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement System Monitoring and Logging",
          "description": "Develop and deploy monitoring endpoints to track system resource usage, and add comprehensive logging for critical operations and errors.",
          "status": "done",
          "dependencies": [],
          "details": "Identify key system resources and operations to monitor (CPU, memory, disk, network, LightRAG performance metrics). Select or extend monitoring tools to expose these metrics via endpoints (e.g., REST, Prometheus). Integrate logging libraries to capture critical events and errors, ensuring logs are structured and accessible for troubleshooting. Define alerting thresholds and procedures for incident response. Validate monitoring and logging coverage through proof-of-concept testing and adjust based on feedback.[3][5]"
        },
        {
          "id": 2,
          "title": "Implement Backup and Data Replication Utilities",
          "description": "Design and implement the backup strategy as outlined in the PRD, including regular snapshots of user data directories, database backups, and utilities for data replication across availability zones.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Review the PRD to extract backup frequency, retention, and recovery requirements. Develop automated scripts or services to perform scheduled snapshots of user data and database backups. Implement utilities to replicate backups across multiple availability zones for redundancy. Ensure backup processes are logged and monitored, and test recovery procedures to verify data integrity and restore capabilities."
        },
        {
          "id": 3,
          "title": "Add Configuration and Performance Tuning Options for LightRAG",
          "description": "Expose configuration options for tuning LightRAG parameters (chunk sizes, embedding model parameters, graph traversal depth) and implement performance monitoring for LightRAG operations.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Extend the system configuration interface (e.g., config files, environment variables, admin UI) to allow dynamic adjustment of LightRAG parameters. Integrate performance monitoring hooks specific to LightRAG operations, capturing metrics such as query latency, throughput, and resource consumption. Ensure changes to configuration are logged and can be rolled back if needed. Provide documentation and guidelines for tuning based on observed performance metrics."
        }
      ]
    },
    {
      "id": 11,
      "title": "Implement Text Ingestion API Endpoints for LightRAG",
      "description": "Create API endpoints that allow users to ingest plain text directly into LightRAG without requiring file uploads, similar to the existing document upload functionality.",
      "details": "This task involves extending the EmbedIQ backend to support direct text ingestion into LightRAG instances. The implementation should follow these steps:\n\n1. Create a new `ingest_text` function in `lightrag_utils.py` that:\n   - Accepts plain text content, user ID, and optional metadata parameters\n   - Processes the text similar to how documents are processed\n   - Chunks the text appropriately for embedding\n   - Adds the text to the user's LightRAG instance\n   - Returns appropriate status information\n\n2. Implement a new endpoint in the documents router:\n   - Create a POST endpoint at `/api/documents/text` that accepts JSON with text content\n   - The endpoint should accept the following parameters:\n     - `text` (required): The plain text content to ingest\n     - `title` (optional): A title for the text content\n     - `metadata` (optional): Additional metadata for the text\n\n3. Add corresponding methods to the DocumentService class:\n   - Create a `ingest_text_content` method that handles the business logic\n   - Ensure proper user context is maintained\n   - Handle any necessary preprocessing of text\n\n4. Implement proper validation and error handling:\n   - Validate text is not empty\n   - Check for maximum text length limits\n   - Handle and return appropriate error responses\n   - Ensure authentication and authorization checks\n\n5. Update API documentation:\n   - Add the new endpoint to the OpenAPI/Swagger documentation\n   - Include example requests and responses\n   - Document all parameters and response codes\n\nThe implementation should maintain consistency with the existing document ingestion flow while simplifying the process for plain text content.",
      "testStrategy": "Testing should verify the text ingestion functionality works correctly and integrates properly with the existing system:\n\n1. Unit tests:\n   - Test the `ingest_text` function with various text inputs (short text, long text, special characters)\n   - Test validation logic for empty text and oversized content\n   - Test metadata handling and optional parameters\n\n2. Integration tests:\n   - Test the API endpoint with authenticated requests\n   - Verify text is properly ingested into the correct user's LightRAG instance\n   - Test concurrent text ingestion requests\n\n3. Error handling tests:\n   - Test authentication failures\n   - Test validation errors (empty text, excessive length)\n   - Test system errors (database unavailable, etc.)\n\n4. End-to-end tests:\n   - Ingest text and verify it can be retrieved in search results\n   - Verify text content is properly chunked and embedded\n   - Test that ingested text works correctly in RAG queries\n\n5. Performance tests:\n   - Measure ingestion time for various text lengths\n   - Test system behavior under load with multiple text ingestion requests\n\nAll tests should verify proper HTTP status codes, response formats, and error messages where applicable.",
      "status": "pending",
      "dependencies": [],
      "priority": "medium"
    }
  ],
  "metadata": {
    "projectName": "EmbedIQ Backend Implementation",
    "totalTasks": 10,
    "sourceFile": "task-master/prd.md",
    "generatedAt": "2023-11-09"
  }
}